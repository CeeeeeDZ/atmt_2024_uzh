INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/with_batch_1000/checkpoints --log-file assignments/03/with_batch_1000/exp.log --batch-size 1000 --cuda
INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': 'assignments/03/with_batch_1000/exp.log', 'save_dir': 'assignments/03/with_batch_1000/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 6000 words
INFO: Loaded a target dictionary (en) with 6000 words
INFO: Built a model with 1822576 parameters
INFO: Epoch 000: loss 8.661 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 2.562 | clip 0.1333
INFO: Epoch 000: valid_loss 8.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 5.19e+03
INFO: Epoch 001: loss 8.373 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.113 | clip 0.6
INFO: Epoch 001: valid_loss 7.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.25e+03
INFO: Epoch 002: loss 7.272 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 15.48 | clip 1
INFO: Epoch 002: valid_loss 6.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 641
INFO: Epoch 003: loss 6.34 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 15.58 | clip 1
INFO: Epoch 003: valid_loss 5.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 329
INFO: Epoch 004: loss 5.893 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 13.61 | clip 1
INFO: Epoch 004: valid_loss 5.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 246
INFO: Epoch 005: loss 5.719 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 12.26 | clip 0.7333
INFO: Epoch 005: valid_loss 5.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 221
INFO: Epoch 006: loss 5.651 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.92 | clip 0.6
INFO: Epoch 006: valid_loss 5.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 212
INFO: Epoch 007: loss 5.62 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.65 | clip 0.7333
INFO: Epoch 007: valid_loss 5.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 205
INFO: Epoch 008: loss 5.587 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.32 | clip 0.7333
INFO: Epoch 008: valid_loss 5.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 198
INFO: Epoch 009: loss 5.553 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.35 | clip 0.8
INFO: Epoch 009: valid_loss 5.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 191
INFO: Epoch 010: loss 5.519 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.38 | clip 0.8
INFO: Epoch 010: valid_loss 5.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 184
INFO: Epoch 011: loss 5.483 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.18 | clip 0.8
INFO: Epoch 011: valid_loss 5.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 178
INFO: Epoch 012: loss 5.448 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.16 | clip 0.8
INFO: Epoch 012: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 171
INFO: Epoch 013: loss 5.408 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10 | clip 0.8
INFO: Epoch 013: valid_loss 5.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 164
INFO: Epoch 014: loss 5.371 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.715 | clip 0.8
INFO: Epoch 014: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 157
INFO: Epoch 015: loss 5.337 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.644 | clip 0.7333
INFO: Epoch 015: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 152
INFO: Epoch 016: loss 5.307 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.392 | clip 0.7333
INFO: Epoch 016: valid_loss 4.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 147
INFO: Epoch 017: loss 5.276 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.17 | clip 0.6667
INFO: Epoch 017: valid_loss 4.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 142
INFO: Epoch 018: loss 5.247 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.807 | clip 0.6
INFO: Epoch 018: valid_loss 4.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 138
INFO: Epoch 019: loss 5.219 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.469 | clip 0.5333
INFO: Epoch 019: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 134
INFO: Epoch 020: loss 5.189 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.048 | clip 0.5333
INFO: Epoch 020: valid_loss 4.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 130
INFO: Epoch 021: loss 5.16 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.554 | clip 0.4667
INFO: Epoch 021: valid_loss 4.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 128
INFO: Epoch 022: loss 5.138 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.548 | clip 0.4667
INFO: Epoch 022: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126
INFO: Epoch 023: loss 5.118 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.79 | clip 0.5333
INFO: Epoch 023: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121
INFO: Epoch 024: loss 5.087 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.689 | clip 0.4667
INFO: Epoch 024: valid_loss 4.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 118
INFO: Epoch 025: loss 5.059 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.903 | clip 0.5333
INFO: Epoch 025: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116
INFO: Epoch 026: loss 5.047 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.832 | clip 0.8667
INFO: Epoch 026: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 127
INFO: Epoch 027: loss 5.102 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 10.58 | clip 0.8
INFO: Epoch 027: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 113
INFO: Epoch 028: loss 5.011 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.135 | clip 0.4667
INFO: Epoch 028: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109
INFO: Epoch 029: loss 4.983 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.336 | clip 0.5333
INFO: Epoch 029: valid_loss 4.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108
INFO: Epoch 030: loss 4.97 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.358 | clip 0.8
INFO: Epoch 030: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112
INFO: Epoch 031: loss 4.987 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.658 | clip 0.8667
INFO: Epoch 031: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106
INFO: Epoch 032: loss 4.935 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.787 | clip 0.6
INFO: Epoch 032: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102
INFO: Epoch 033: loss 4.922 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.763 | clip 0.7333
INFO: Epoch 033: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106
INFO: Epoch 034: loss 4.941 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.454 | clip 0.8
INFO: Epoch 034: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101
INFO: Epoch 035: loss 4.881 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.946 | clip 0.6
INFO: Epoch 035: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.2
INFO: Epoch 036: loss 4.868 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.756 | clip 0.7333
INFO: Epoch 036: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.7
INFO: Epoch 037: loss 4.87 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.203 | clip 0.6667
INFO: Epoch 037: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.1
INFO: Epoch 038: loss 4.827 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.584 | clip 0.4
INFO: Epoch 038: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.7
INFO: Epoch 039: loss 4.808 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.411 | clip 0.6667
INFO: Epoch 039: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94.2
INFO: Epoch 040: loss 4.821 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.872 | clip 0.7333
INFO: Epoch 040: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.4
INFO: Epoch 041: loss 4.778 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.247 | clip 0.4
INFO: Epoch 041: valid_loss 4.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.8
INFO: Epoch 042: loss 4.756 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.505 | clip 0.6
INFO: Epoch 042: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.7
INFO: Epoch 043: loss 4.748 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.389 | clip 0.6
INFO: Epoch 043: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.2
INFO: Epoch 044: loss 4.747 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.019 | clip 0.7333
INFO: Epoch 044: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.9
INFO: Epoch 045: loss 4.711 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.651 | clip 0.7333
INFO: Epoch 045: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.6
INFO: Epoch 046: loss 4.703 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.053 | clip 0.8
INFO: Epoch 046: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.9
INFO: Epoch 047: loss 4.726 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.669 | clip 1
INFO: Epoch 047: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.1
INFO: Epoch 048: loss 4.685 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.059 | clip 0.8
INFO: Epoch 048: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.3
INFO: Epoch 049: loss 4.661 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.912 | clip 0.4
INFO: Epoch 049: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.7
INFO: Epoch 050: loss 4.632 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.426 | clip 0.3333
INFO: Epoch 050: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77
INFO: Epoch 051: loss 4.609 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.023 | clip 0.2667
INFO: Epoch 051: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.2
INFO: Epoch 052: loss 4.603 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.07 | clip 0.4
INFO: Epoch 052: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.5
INFO: Epoch 053: loss 4.58 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.381 | clip 0.3333
INFO: Epoch 053: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.1
INFO: Epoch 054: loss 4.56 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.69 | clip 0.4
INFO: Epoch 054: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.2
INFO: Epoch 055: loss 4.542 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.4 | clip 0.6
INFO: Epoch 055: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.4
INFO: Epoch 056: loss 4.538 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.055 | clip 0.7333
INFO: Epoch 056: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.2
INFO: Epoch 057: loss 4.543 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.901 | clip 0.8667
INFO: Epoch 057: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.9
INFO: Epoch 058: loss 4.507 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.553 | clip 0.6
INFO: Epoch 058: valid_loss 4.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.4
INFO: Epoch 059: loss 4.513 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.651 | clip 0.8
INFO: Epoch 059: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.1
INFO: Epoch 060: loss 4.531 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.307 | clip 1
INFO: Epoch 060: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.6
INFO: Epoch 061: loss 4.482 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.86 | clip 0.8
INFO: Epoch 061: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.1
INFO: Epoch 062: loss 4.463 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.824 | clip 0.4
INFO: Epoch 062: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.8
INFO: Epoch 063: loss 4.453 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.224 | clip 0.5333
INFO: Epoch 063: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.3
INFO: Epoch 064: loss 4.424 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.828 | clip 0.5333
INFO: Epoch 064: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.2
INFO: Epoch 065: loss 4.412 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.25 | clip 0.6667
INFO: Epoch 065: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.3
INFO: Epoch 066: loss 4.418 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.535 | clip 0.6667
INFO: Epoch 066: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.3
INFO: Epoch 067: loss 4.384 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.252 | clip 0.2667
INFO: Epoch 067: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.5
INFO: Epoch 068: loss 4.374 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.223 | clip 0.6
INFO: Epoch 068: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.5
INFO: Epoch 069: loss 4.363 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.456 | clip 0.7333
INFO: Epoch 069: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.1
INFO: Epoch 070: loss 4.386 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.267 | clip 0.8667
INFO: Epoch 070: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.4
INFO: Epoch 071: loss 4.358 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.823 | clip 0.8
INFO: Epoch 071: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.6
INFO: Epoch 072: loss 4.326 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.362 | clip 0.5333
INFO: Epoch 072: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.3
INFO: Epoch 073: loss 4.326 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.875 | clip 0.5333
INFO: Epoch 073: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.2
INFO: Epoch 074: loss 4.308 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.824 | clip 0.6
INFO: Epoch 074: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.2
INFO: Epoch 075: loss 4.296 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.194 | clip 0.7333
INFO: Epoch 075: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.7
INFO: Epoch 076: loss 4.344 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.92 | clip 1
INFO: Epoch 076: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.8
INFO: Epoch 077: loss 4.304 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.097 | clip 0.8
INFO: Epoch 077: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.4
INFO: Epoch 078: loss 4.276 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.834 | clip 0.5333
INFO: Epoch 078: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.9
INFO: Epoch 079: loss 4.259 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.515 | clip 0.7333
INFO: Epoch 079: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.5
INFO: Epoch 080: loss 4.249 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.251 | clip 0.4
INFO: Epoch 080: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4
INFO: Epoch 081: loss 4.247 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.243 | clip 0.5333
INFO: Epoch 081: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.6
INFO: Epoch 082: loss 4.225 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.861 | clip 0.6
INFO: Epoch 082: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.6
INFO: Epoch 083: loss 4.219 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.189 | clip 0.7333
INFO: Epoch 083: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7
INFO: Epoch 084: loss 4.259 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.835 | clip 1
INFO: Epoch 084: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4
INFO: Epoch 085: loss 4.224 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.779 | clip 0.8
INFO: Epoch 085: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52
INFO: Epoch 086: loss 4.196 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.697 | clip 0.4667
INFO: Epoch 086: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52
INFO: Epoch 087: loss 4.176 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.152 | clip 0.6
INFO: Epoch 087: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.1
INFO: Epoch 088: loss 4.17 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.534 | clip 0.3333
INFO: Epoch 088: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.8
INFO: Epoch 089: loss 4.186 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.548 | clip 0.8667
INFO: Epoch 089: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.4
INFO: Epoch 090: loss 4.164 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.761 | clip 0.6667
INFO: Epoch 090: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.8
INFO: Epoch 091: loss 4.146 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.533 | clip 0.6
INFO: Epoch 091: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.5
INFO: Epoch 092: loss 4.164 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.137 | clip 0.9333
INFO: Epoch 092: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.6
INFO: Epoch 093: loss 4.134 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.287 | clip 0.6667
INFO: Epoch 093: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.9
INFO: Epoch 094: loss 4.119 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.533 | clip 0.4
INFO: Epoch 094: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49
INFO: Epoch 095: loss 4.125 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.931 | clip 0.7333
INFO: Epoch 095: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.9
INFO: Epoch 096: loss 4.106 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.904 | clip 0.5333
INFO: Epoch 096: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.7
INFO: Epoch 097: loss 4.094 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.174 | clip 0.6
INFO: Epoch 097: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.6
INFO: Epoch 098: loss 4.116 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.448 | clip 1
INFO: Epoch 098: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.5
INFO: Epoch 099: loss 4.091 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.432 | clip 0.7333
INFO: Epoch 099: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.9
INFO: Epoch 100: loss 4.067 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.743 | clip 0.4
INFO: Epoch 100: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.4
INFO: Epoch 101: loss 4.059 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.648 | clip 0.2
INFO: Epoch 101: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.8
INFO: Epoch 102: loss 4.049 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.546 | clip 0.4667
INFO: Epoch 102: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.7
INFO: Epoch 103: loss 4.05 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.495 | clip 0.6667
INFO: Epoch 103: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47
INFO: Epoch 104: loss 4.075 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.73 | clip 1
INFO: Epoch 104: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.7
INFO: Epoch 105: loss 4.033 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.318 | clip 0.6667
INFO: Epoch 105: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45
INFO: Epoch 106: loss 4.017 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.023 | clip 0.4
INFO: Epoch 106: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.6
INFO: Epoch 107: loss 4.01 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.012 | clip 0.3333
INFO: Epoch 107: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.3
INFO: Epoch 108: loss 3.995 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.608 | clip 0.5333
INFO: Epoch 108: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.7
INFO: Epoch 109: loss 3.993 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.484 | clip 0.6
INFO: Epoch 109: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9
INFO: Epoch 110: loss 4.028 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.291 | clip 1
INFO: Epoch 110: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9
INFO: Epoch 111: loss 3.993 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 6.54 | clip 0.7333
INFO: Epoch 111: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.4
INFO: Epoch 112: loss 3.979 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.897 | clip 0.6
INFO: Epoch 112: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7
INFO: Epoch 113: loss 3.954 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 4.187 | clip 0.4
INFO: Epoch 113: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7
INFO: Epoch 114: loss 3.948 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.194 | clip 0.6667
INFO: Epoch 114: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8
INFO: Epoch 115: loss 3.961 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 8.326 | clip 0.6667
INFO: Epoch 115: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6
INFO: Epoch 116: loss 3.933 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 5.225 | clip 0.4
INFO: Epoch 116: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42
INFO: Epoch 117: loss 3.946 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 9.357 | clip 0.7333
INFO: Epoch 117: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7
INFO: Epoch 118: loss 3.964 | lr 0.0003 | num_tokens 12.74 | batch_size 1000 | grad_norm 7.527 | clip 1
INFO: Epoch 118: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.9
INFO: No validation set improvements observed for 3 epochs. Early stop!
